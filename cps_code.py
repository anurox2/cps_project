# -*- coding: utf-8 -*-
"""CPS_Project_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17ZCjzqJn_zzV1kf95-QWEgWOg8B8CKvy
"""

# Import libraries here
## Data manipulation libraries
import pandas as pd
import numpy as np
from numpy import savetxt

## Machine learning libraries
from tensorflow.python.keras.preprocessing import sequence
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers.core import Dense, Dropout, Activation
from tensorflow.python.keras.layers.embeddings import Embedding
from tensorflow.python.keras.layers.recurrent import LSTM
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import classification_report
from matplotlib import pyplot as plt

"""## Data Prep"""

# Read the benign dataset and add a classification column
df1_benign = pd.read_csv('top_1m_small.csv', index_col=False, header=None, low_memory=False)
df1_benign.drop(columns={0}, inplace=True)
df1_benign.rename(columns={1:'URLs'}, inplace=True)

# Benign URLs are set to 0 (zero)
df1_benign['classify'] = 0

# Equalizing records
#df1_benign = df1_benign.sample(frac=1, random_state=10)
#df1_benign = df1_benign.head(847026)

# Read the malicious dataset and add a classification column
df2_mal = pd.read_csv('dga_text.csv', header=None, low_memory=False)
df2_mal.rename(columns={0: 'URLs'}, inplace=True)

# Malicious URLs are set to 1 (one)
df2_mal['classify'] = 1

df2_mal.shape

df1_benign.shape

# Forming the dataset
df3_final = pd.concat([df1_benign, df2_mal], axis=0)
df3_final = df3_final.sample(frac=1).reset_index(drop=True)

## Writing the final dataset back
# df3_final.to_csv('final_dataset.csv', index=False)

"""## Machine Learning"""

## Using only a subset of the data
#df4_test = df3_final.head(10000)
df4_test = df3_final.copy(deep=True)

X = df4_test['URLs'].tolist()

valid_chars = {x:idx+1 for idx, x in enumerate(set(''.join(X)))}
max_features = len(valid_chars)+1
maxlen = np.max([len(x) for x in X])

X = [[valid_chars[y] for y in x] for x in X]
X = sequence.pad_sequences(X, maxlen=maxlen)

y = df4_test['classify'].values
labels = ['URLs', 'classify']

# ML model

epochs = 1
ml_model1 = Sequential()

ml_model1.add(Embedding(max_features, 128, input_length=maxlen))
ml_model1.add(LSTM(128))
ml_model1.add(Dropout(0.5))
ml_model1.add(Dense(1))
ml_model1.add(Activation('sigmoid'))

ml_model1.compile(loss='binary_crossentropy', optimizer='rmsprop')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,)

history = ml_model1.fit(X_train, y_train, batch_size=64, epochs=epochs, validation_data=(X_test, y_test))

pred = ml_model1.predict(X_test)
out_data = {'y':y_test, 'pred':pred, 'confusion_matrix': sklearn.metrics.confusion_matrix(y_test, pred>0.5)}
print("\n\nConfusion Matrix",sklearn.metrics.confusion_matrix(y_test, pred>0.5))
print("\n\nAccuracy of the model", accuracy_score(y_test, pred>0.5)*100)


## Get training and testing loss values

training_loss = history.history['loss']
test_loss = history.history['val_loss']

## Create count of the number of epochs
epoch_count = range(1, len(training_loss)+1)

## Visualize loss history
plt.plot(epoch_count, training_loss, 'r--')
plt.plot(epoch_count, test_loss, 'b-')
plt.legend(['Training Loss', 'Test Loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.savefig('second_run.pdf', bbox_inches='tight')
